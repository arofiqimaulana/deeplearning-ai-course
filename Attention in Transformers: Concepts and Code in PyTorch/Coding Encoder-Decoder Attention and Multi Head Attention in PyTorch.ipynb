{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44339fbf-eea7-487b-ab67-e308fea658bb",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Apa Itu Encoder-Decoder Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d92ca3-6a04-442e-939f-d02eb1790165",
   "metadata": {},
   "source": [
    "## Terminologi\n",
    "\n",
    "- Encoder: Berasal dari kata \"encode\" yang berarti **mengubah informasi dari bentuk yang mudah dimengerti menjadi format yang lebih terstruktur atau lebih tersembunyi, seperti representasi vektor**. Encoder mengubah input menjadi representasi internal yang lebih mudah diproses oleh model.\n",
    "\n",
    "\n",
    "- Decoder: Berasal dari kata \"decode\" yang berarti **mengubah kembali representasi yang tersembunyi atau terstruktur menjadi informasi yang dapat dipahami atau digunakan, seperti menghasilkan kalimat yang bisa dipahami manusia**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9093fe-aa4b-41bc-b304-e2d13ea0efd9",
   "metadata": {},
   "source": [
    "## Perbandingan\n",
    "- Encoder-only model digunakan untuk **pemahaman input (menganalisis dan memproses data)**, sedangkan decoder-only model digunakan untuk **generasi output (membuat atau meramalkan teks baru)**.\n",
    "\n",
    "\n",
    "- Encoder-only lebih cocok untuk **tugas pemahaman** (seperti klasifikasi teks, pengenalan entitas), sementara decoder-only lebih cocok untuk **tugas generasi** (seperti pembuatan teks otomatis, penerjemahan bahasa)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a065c62-5ea4-4aae-9194-94cfaf462d7a",
   "metadata": {},
   "source": [
    "| **Model**            | **Tugas Utama**                                     | **Kelebihan**                                               | **Kekurangan**                                              | **Contoh Model**          |\n",
    "|----------------------|-----------------------------------------------------|-------------------------------------------------------------|-------------------------------------------------------------|---------------------------|\n",
    "| **Decoder-Only**      | Generasi teks (autoregressive)                     | - Sangat efektif untuk generasi teks                        | - Tidak optimal untuk klasifikasi teks                      | GPT, GPT-2, GPT-3, GPT-4  |\n",
    "|                      | (misalnya: teks bebas, dialog, QA)                  | - Bisa digunakan untuk tugas generasi teks                  | - Tidak dirancang untuk memahami konteks dua arah           |                           |\n",
    "|                      |                                                     | - Dapat digunakan untuk klasifikasi dengan penyesuaian      | - Memerlukan modifikasi (layer klasifikasi tambahan)        |                           |\n",
    "| **Encoder-Only**      | Klasifikasi teks, pemahaman teks                   | - Sangat efektif untuk klasifikasi teks dan pemahaman konteks| - Tidak dirancang untuk generasi teks bebas                 | BERT, RoBERTa, DistilBERT |\n",
    "|                      | (misalnya: analisis sentimen, ekstraksi informasi)  | - Memproses teks dalam konteks dua arah (baik kiri-kanan)    | - Tidak cocok untuk tugas generasi teks bebas               |                           |\n",
    "|                      |                                                     | - Memberikan representasi kontekstual yang kuat             |                                                             |                           |\n",
    "| **Encoder-Decoder**   | Penerjemahan mesin, summarization, transformasi teks| - Menggabungkan kekuatan encoder untuk pemahaman dan decoder untuk generasi | - Lebih kompleks dan memerlukan lebih banyak sumber daya    | Transformer, T5, BART     |\n",
    "|                      | (misalnya: teks ke teks, terjemahan)                | - Cocok untuk tugas yang memerlukan pemahaman dan generasi  | - Tidak seefisien encoder-only untuk klasifikasi teks       |                           |\n",
    "|                      |                                                     | - Mampu menangani tugas kompleks yang memerlukan transformasi |                                                             |                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479efc7a-cd2b-4993-af53-17487c3b531e",
   "metadata": {},
   "source": [
    "## Apakah Regresi Logistik termasuk Decoder-Only ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2ba677-c7ef-4bcb-b63c-fc6518111051",
   "metadata": {},
   "source": [
    "Analisis Regresi Logistik bukan termasuk model Decoder-Only karena tidak adanya elemen-elemen penting yang ada dalam model encoder, seperti \n",
    "- **Word embeddings**\n",
    "- **Positional encoding**\n",
    "- **Mechanism attention (Q, K, V)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "593bd147-1e56-49b6-b4d3-fa57931f8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch ## torch let's us create tensors and also provides helper functions\n",
    "import torch.nn as nn ## torch.nn gives us nn.module() and nn.Linear()\n",
    "import torch.nn.functional as F # This gives us the softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71773355-a239-4713-bde5-0e1e39c8913a",
   "metadata": {},
   "source": [
    "# Code Attention\n",
    "<a id=\"attention\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "846a4e0c-c473-48eb-973e-038a6ea77fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module): \n",
    "                            \n",
    "    def __init__(self, d_model=2,  \n",
    "                 row_dim=0, \n",
    "                 col_dim=1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        \n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "\n",
    "\n",
    "    ## The only change from SelfAttention and attention is that\n",
    "    ## now we expect 3 sets of encodings to be passed in...\n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
    "        ## ...and we pass those sets of encodings to the various weight matrices.\n",
    "        q = self.W_q(encodings_for_q)\n",
    "        k = self.W_k(encodings_for_k)\n",
    "        v = self.W_v(encodings_for_v)\n",
    "\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "\n",
    "        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "            \n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc4801d-fa03-4ee8-9613-7716707eff1a",
   "metadata": {},
   "source": [
    "# Calculate Encoder-Decoder Attention\n",
    "<a id=\"calculate\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "040320fe-be5f-4dc0-bfab-af411b3085a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create matrices of token encodings...\n",
    "encodings_for_q = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "encodings_for_k = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "encodings_for_v = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "## set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "## create an attention object\n",
    "attention = Attention(d_model=2,\n",
    "                      row_dim=0,\n",
    "                      col_dim=1)\n",
    "\n",
    "## calculate encoder-decoder attention\n",
    "attention(encodings_for_q, encodings_for_k, encodings_for_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6c5e2f-88c4-4ab5-977b-063d2e3ac781",
   "metadata": {},
   "source": [
    "# Code Mutli-Head Attention\n",
    "<a id=\"multi\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e1613eb-7e47-4272-a56a-d12dbaf40930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 d_model=2,  \n",
    "                 row_dim=0, \n",
    "                 col_dim=1, \n",
    "                 num_heads=1):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        ## create a bunch of attention heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Attention(d_model, row_dim, col_dim) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "        self.col_dim = col_dim\n",
    "        \n",
    "    def forward(self, \n",
    "                encodings_for_q, \n",
    "                encodings_for_k,\n",
    "                encodings_for_v):\n",
    "\n",
    "        ## run the data through all of the attention heads\n",
    "        return torch.cat([head(encodings_for_q, \n",
    "                               encodings_for_k,\n",
    "                               encodings_for_v) \n",
    "                          for head in self.heads], dim=self.col_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fce3057-6d1c-459d-abf9-3cea4a5c3555",
   "metadata": {},
   "source": [
    "# Calcualte Multi-Head Attention\n",
    "<a id=\"calcMulti\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "212f33e5-b990-49f5-87b5-97c72c9531e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "## create an attention object\n",
    "multiHeadAttention = MultiHeadAttention(d_model=2,\n",
    "                                        row_dim=0,\n",
    "                                        col_dim=1,\n",
    "                                        num_heads=1)\n",
    "\n",
    "## calculate encoder-decoder attention\n",
    "multiHeadAttention(encodings_for_q, encodings_for_k, encodings_for_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de723576-b880-4dcb-b9a5-babe9df1e485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0100,  1.0641, -0.7081, -0.8268],\n",
       "        [ 0.2040,  0.7057, -0.7417, -0.9193],\n",
       "        [ 3.4989,  2.2427, -0.7190, -0.8447]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "## create an attention object\n",
    "multiHeadAttention = MultiHeadAttention(d_model=2,\n",
    "                                        row_dim=0,\n",
    "                                        col_dim=1,\n",
    "                                        num_heads=2)\n",
    "\n",
    "## calculate encoder-decoder attention\n",
    "multiHeadAttention(encodings_for_q, encodings_for_k, encodings_for_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb722187-e87f-4322-ac60-2da576ba0fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
